{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloudfare - Inverted Index Solution by Vamshi Krishna Terala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Steps:\n",
    "    1. Create a document dictionary. This document dictionary has an unique id assigned to each document. Broadcast the document dictionary\n",
    "        (filename, file_index)\n",
    "    2. Create a word dictionary. This dictionary has the list of all words and each word is assigned a unique id. Broadcast the word dictionary\n",
    "        (word, word_index)\n",
    "    3. Use the above dictionaries to get the RDD like below\n",
    "        (word_index, file_index)\n",
    "    4. ReduceByKey and Sort by word id and then document ids to get the inverted index\n",
    "        (word_index, [list of file_indexes])\n",
    "    5. Save the output to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "sparkconf = SparkConf().setAppName(\"InvIndex\")\n",
    "sc = SparkContext.getOrCreate(conf=sparkconf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Vamshi/Cloudfare/test/data\\\\file0.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file1.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file10.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file11.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file12.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file13.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file14.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file15.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file16.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file17.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file18.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file19.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file2.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file20.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file21.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file22.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file23.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file24.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file3.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file4.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file5.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file6.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file7.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file8.txt',\n",
       " 'C:/Vamshi/Cloudfare/test/data\\\\file9.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob, os\n",
    "base_files = glob.glob(\"C:/Vamshi/Cloudfare/test/data/*.txt\")\n",
    "base_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create a document dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Zip function will assign a index to each document.\n",
    "    Broadcast this dictionary to reducers as this is small in size. This improves the performance of the final reduce phase.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_indexes = [i for i in range(len(base_files))]\n",
    "document_dict = dict(zip(base_files, document_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_dict_broadcast = sc.broadcast(document_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Generator Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Create a list of words in the file.\n",
    "    Map each word to the source file name.\n",
    "    Final Output: (word, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def word_generator(filename):\n",
    "    with open(filename, 'r', encoding=\"UTF-8\") as f:\n",
    "        words=[]\n",
    "        for line in f:\n",
    "            line = re.sub(r'[^\\w\\s]','',line)\n",
    "            words = words + line.strip().split()\n",
    "        words_dict = {(s,filename) for s in words}\n",
    "        return words_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parallelize the read files and call the word generator function on each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sense', 'C:/Vamshi/Cloudfare/test/data\\\\file0.txt'),\n",
       " ('three', 'C:/Vamshi/Cloudfare/test/data\\\\file0.txt'),\n",
       " ('draught', 'C:/Vamshi/Cloudfare/test/data\\\\file0.txt'),\n",
       " ('endeavours', 'C:/Vamshi/Cloudfare/test/data\\\\file0.txt'),\n",
       " ('irrevocably', 'C:/Vamshi/Cloudfare/test/data\\\\file0.txt')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd = sc.parallelize(base_files).flatMap(word_generator)\n",
    "words_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a word dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    From the above RDD get all the words(keys)\n",
    "    Zip with index gives each word an index value starting from first word in the first partition to the last word in the last partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_word_dictionary = words_rdd\\\n",
    "    .map(lambda x: x[0])\\\n",
    "    .zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sense', 0),\n",
       " ('three', 1),\n",
       " ('draught', 2),\n",
       " ('endeavours', 3),\n",
       " ('irrevocably', 4)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_word_dictionary.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The above indexed dictionary contains duplicates.\n",
    "    We can use distinct to remove duplicates but it will trigger a shuffle. \n",
    "    Instead, use collectAsMap which removes all the duplicates and returns the final dictionary to the driver.\n",
    "    collectAsMap function does not trigger a shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_word_dictionary = indexed_word_dictionary.collectAsMap()\n",
    "word_dictionary_broadcast = sc.broadcast(dedup_word_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Map document dictionary and word dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    RDD: (word, filename)\n",
    "    Document dictionary (broadcast): (filename, file_index)\n",
    "    Word dictionary (broadcast): (word, word_index)\n",
    "    \n",
    "    1. Map initial RDD and document dictionary to create a rdd of the form (word, file_index)\n",
    "    2. Map the above rdd with word dictionary to create a rdd of the form (word_index, file_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_id(w):\n",
    "    return (w[0],document_dict_broadcast.value[w[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_rdd_document = words_rdd.map(lambda w : get_document_id(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sense', 0),\n",
       " ('three', 0),\n",
       " ('draught', 0),\n",
       " ('endeavours', 0),\n",
       " ('irrevocably', 0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd_document.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordid_documentid = words_rdd_document.map(lambda w: (word_dictionary_broadcast.value[w[0]], w[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(24735, 0),\n",
       " (22805, 0),\n",
       " (17802, 0),\n",
       " (21724, 0),\n",
       " (3371, 0),\n",
       " (25419, 0),\n",
       " (14720, 0),\n",
       " (19825, 0),\n",
       " (23010, 0),\n",
       " (19706, 0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordid_documentid.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create Inverted index and Sort it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. ReduceByKey to get rdd of the form (word_index, [list of file_indexes])\n",
    "    2. SortByKey to sort the inverted index based on word_indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = wordid_documentid.map(lambda w : (w[0], [w[1]])).reduceByKey(lambda x,y : x+y).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, [0]),\n",
       " (19, [0]),\n",
       " (20, [0]),\n",
       " (30, [0]),\n",
       " (39, [0]),\n",
       " (41, [0]),\n",
       " (49, [0]),\n",
       " (51, [0]),\n",
       " (65, [0]),\n",
       " (67, [0])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save the output to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index.saveAsTextFile(\"cloudfare_inverted_index_result.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
